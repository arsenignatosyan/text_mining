{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dfdbcd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1b0a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arsen/miniconda3/envs/text_min/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, \\\n",
    "                         TrainingArguments, Trainer\n",
    "from torch.optim import AdamW\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4033aa3",
   "metadata": {},
   "source": [
    "# Load and Process Dataset\n",
    "\n",
    "We start by loading the conll data via the hugging face api and its load_dataset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17dcab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1535e282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bca283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@paulwalk',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'Empire',\n",
       " 'State',\n",
       " 'Building',\n",
       " '=',\n",
       " 'ESB',\n",
       " '.',\n",
       " 'Pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e811ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde37674",
   "metadata": {},
   "source": [
    "Followingly, we extract the named entitiy recognition features and the respective IOB labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a34c0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794bb5e",
   "metadata": {},
   "source": [
    "We see that the train body features labels about corporations, creative-work, groups, locations, perople, and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060450fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05f4f9",
   "metadata": {},
   "source": [
    "Based on this, we can express the decoded information and a respective sentence jointly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b3493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@paulwalk It 's the view from where I 'm living for two weeks . Empire     State      Building   = ESB        . Pretty bad storm here last evening . \n",
      "O         O  O  O   O    O    O     O O  O      O   O   O     O B-location I-location I-location O B-location O O      O   O     O    O    O       O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782581a",
   "metadata": {},
   "source": [
    "We will use a pretrained bert model to evaluate the contents of the WNUT dataset. This dataset features a lot of rare entities and thereby allows for testing models on largely unseen information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9dcd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9c37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e339a03",
   "metadata": {},
   "source": [
    "Next, we will give the tokens new labels which align with their new labels, which express their purpose numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be042439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd0ac737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5408c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05621643",
   "metadata": {},
   "source": [
    "Having composed a function to tokenise and align the labels, we finally arrive at a preprocessed and tokenised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b26d1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35970fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3a426",
   "metadata": {},
   "source": [
    "# Setting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae3f7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdd29742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ec27879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_token_classification(logits, labels, entity_types):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "    f1_dict = {}\n",
    "    support_dict = {}\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        precision_dict[entity_type] = {}\n",
    "        recall_dict[entity_type] = {}\n",
    "        f1_dict[entity_type] = {}\n",
    "        support_dict[entity_type] = {}\n",
    "        b_true_labels_binary = [[l if f\"B-{entity_type}\" == l else 'O' for l in label] for label in true_labels]\n",
    "        b_pred_labels_binary = [[l if f\"B-{entity_type}\" == l else 'O' for l in label] for label in predictions]\n",
    "        \n",
    "        b_metrics = metric.compute(predictions=b_pred_labels_binary, references=b_true_labels_binary)\n",
    "        \n",
    "        precision_dict[entity_type]['B-label'] = b_metrics[entity_type]['precision']\n",
    "        recall_dict[entity_type]['B-label'] = b_metrics[entity_type]['recall']\n",
    "        f1_dict[entity_type]['B-label'] = b_metrics[entity_type]['f1']\n",
    "        support_dict[entity_type]['B-label'] = b_metrics[entity_type]['number']\n",
    "        \n",
    "        i_true_labels_binary = [[l if f\"I-{entity_type}\" == l else 'O' for l in label] for label in true_labels]\n",
    "        i_pred_labels_binary = [[l if f\"I-{entity_type}\" == l else 'O' for l in label] for label in predictions]\n",
    "    \n",
    "        i_metrics = metric.compute(predictions=i_pred_labels_binary, references=i_true_labels_binary)\n",
    "        \n",
    "        precision_dict[entity_type]['I-label'] = i_metrics[entity_type]['precision']\n",
    "        recall_dict[entity_type]['I-label'] = i_metrics[entity_type]['recall']\n",
    "        f1_dict[entity_type]['I-label'] = i_metrics[entity_type]['f1']\n",
    "        support_dict[entity_type]['I-label'] = i_metrics[entity_type]['number']\n",
    "\n",
    "    entity_metrics = metric.compute(predictions=predictions, references=true_labels)\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        precision_dict[entity_type]['entity'] = entity_metrics[entity_type]['precision']\n",
    "        recall_dict[entity_type]['entity'] = entity_metrics[entity_type]['recall']\n",
    "        f1_dict[entity_type]['entity'] = entity_metrics[entity_type]['f1']\n",
    "        support_dict[entity_type]['entity'] = entity_metrics[entity_type]['number']\n",
    "        \n",
    "    f1_scores_list = [f1_dict[entity_type][\"entity\"] for entity_type in f1_dict] \n",
    "    support_list = [support_dict[entity_type][\"entity\"] for entity_type in support_dict]\n",
    "    weights_support_list = [support / sum(support_list) for support in support_list]\n",
    "    \n",
    "    final_dict = {\n",
    "        \"precision\": precision_dict,\n",
    "        \"recall\": recall_dict,\n",
    "        \"f1\": f1_dict,\n",
    "        \"macro_f1\": sum(f1_scores_list) / len(f1_scores_list),\n",
    "        \"micro_f1\": sum([f1_score * support for f1_score, support in zip(f1_scores_list, weights_support_list)])\n",
    "    }\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df648e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types = ['corporation', 'creative-work', 'group', 'location', 'person', 'product']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6b16d",
   "metadata": {},
   "source": [
    "# Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45ebd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a209cf",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "We will use the bert model with baseline parameters for our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aad48fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1275' max='1275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1275/1275 12:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.042900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.2598603665828705,\n",
       " 'test_precision': 0.49262536873156343,\n",
       " 'test_recall': 0.30954587581093607,\n",
       " 'test_f1': 0.3801935116676152,\n",
       " 'test_accuracy': 0.9336180904522613,\n",
       " 'test_runtime': 101.5164,\n",
       " 'test_samples_per_second': 12.678,\n",
       " 'test_steps_per_second': 1.586}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_baseline = TrainingArguments(\"bert-finetuned-baseline\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "trainer_baseline = Trainer(\n",
    "    model=model,\n",
    "    args=args_baseline,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer_baseline.train()\n",
    "\n",
    "test_pred_baseline = trainer_baseline.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "test_pred_baseline.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d827381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'corporation': {'B-label': 0.23999999999999996,\n",
      "                        'I-label': 0.21487603305785122,\n",
      "                        'entity': 0.19047619047619047},\n",
      "        'creative-work': {'B-label': 0.37948717948717947,\n",
      "                          'I-label': 0.31413612565445026,\n",
      "                          'entity': 0.27488151658767773},\n",
      "        'group': {'B-label': 0.270042194092827,\n",
      "                  'I-label': 0.19883040935672516,\n",
      "                  'entity': 0.2076923076923077},\n",
      "        'location': {'B-label': 0.5886792452830187,\n",
      "                     'I-label': 0.43113772455089816,\n",
      "                     'entity': 0.4981949458483754},\n",
      "        'person': {'B-label': 0.5947611710323574,\n",
      "                   'I-label': 0.44353182751540043,\n",
      "                   'entity': 0.5424739195230999},\n",
      "        'product': {'B-label': 0.2650602409638554,\n",
      "                    'I-label': 0.14689265536723164,\n",
      "                    'entity': 0.13612565445026178}},\n",
      " 'macro_f1': 0.30830742242965214,\n",
      " 'micro_f1': 0.38054897698235307,\n",
      " 'precision': {'corporation': {'B-label': 0.2542372881355932,\n",
      "                               'I-label': 0.19696969696969696,\n",
      "                               'entity': 0.1728395061728395},\n",
      "               'creative-work': {'B-label': 0.6981132075471698,\n",
      "                                 'I-label': 0.4838709677419355,\n",
      "                                 'entity': 0.42028985507246375},\n",
      "               'group': {'B-label': 0.4444444444444444,\n",
      "                         'I-label': 0.29310344827586204,\n",
      "                         'entity': 0.28421052631578947},\n",
      "               'location': {'B-label': 0.6782608695652174,\n",
      "                            'I-label': 0.5,\n",
      "                            'entity': 0.5433070866141733},\n",
      "               'person': {'B-label': 0.8772727272727273,\n",
      "                          'I-label': 0.7152317880794702,\n",
      "                          'entity': 0.7520661157024794},\n",
      "               'product': {'B-label': 0.5641025641025641,\n",
      "                           'I-label': 0.23214285714285715,\n",
      "                           'entity': 0.203125}},\n",
      " 'recall': {'corporation': {'B-label': 0.22727272727272727,\n",
      "                            'I-label': 0.23636363636363636,\n",
      "                            'entity': 0.21212121212121213},\n",
      "            'creative-work': {'B-label': 0.2605633802816901,\n",
      "                              'I-label': 0.23255813953488372,\n",
      "                              'entity': 0.20422535211267606},\n",
      "            'group': {'B-label': 0.19393939393939394,\n",
      "                      'I-label': 0.1504424778761062,\n",
      "                      'entity': 0.16363636363636364},\n",
      "            'location': {'B-label': 0.52,\n",
      "                         'I-label': 0.37894736842105264,\n",
      "                         'entity': 0.46},\n",
      "            'person': {'B-label': 0.44988344988344986,\n",
      "                       'I-label': 0.32142857142857145,\n",
      "                       'entity': 0.42424242424242425},\n",
      "            'product': {'B-label': 0.1732283464566929,\n",
      "                        'I-label': 0.10743801652892562,\n",
      "                        'entity': 0.10236220472440945}}}\n"
     ]
    }
   ],
   "source": [
    "macro_micro_baseline = evaluate_token_classification(logits=test_pred_baseline.predictions, \\\n",
    "                                                     labels=test_pred_baseline.label_ids, \\\n",
    "                                                     entity_types=entity_types)\n",
    "pprint(macro_micro_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f7c9b",
   "metadata": {},
   "source": [
    "Overall, the results are rather poor. Next, we will apply different optimisation approaches to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276bb7e",
   "metadata": {},
   "source": [
    "### AdamW Optimization\n",
    "\n",
    "We begin by implementing the AdamW optimiser, which makes use of the general Adam optimisation approach and adds L2 regularisation via a decay in the parameter weights at each iteration. By implementing weight decay, we ensure that the model is more generaliseable and performs better on unseen data. The hyperparameters are again based on a a premade bert configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1575ea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1275' max='1275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1275/1275 11:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.161831</td>\n",
       "      <td>0.549470</td>\n",
       "      <td>0.372010</td>\n",
       "      <td>0.443652</td>\n",
       "      <td>0.917313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.180891</td>\n",
       "      <td>0.584437</td>\n",
       "      <td>0.422249</td>\n",
       "      <td>0.490278</td>\n",
       "      <td>0.921862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.176216</td>\n",
       "      <td>0.556028</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.508761</td>\n",
       "      <td>0.925769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.23555749654769897,\n",
       " 'test_precision': 0.5247208931419458,\n",
       " 'test_recall': 0.3049119555143652,\n",
       " 'test_f1': 0.3856975381008207,\n",
       " 'test_accuracy': 0.9348994974874372,\n",
       " 'test_runtime': 9.7868,\n",
       " 'test_samples_per_second': 131.504,\n",
       " 'test_steps_per_second': 16.451}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "optimizer1 = AdamW(model.parameters(), lr=2e-5)\n",
    "args_adam1 = TrainingArguments(\n",
    "    \"bert-finetuned-adam1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer_adam1 = Trainer(\n",
    "    model=model,\n",
    "    args=args_adam1,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer1, None)\n",
    ")\n",
    "\n",
    "trainer_adam1.train()\n",
    "test_pred_adam1 = trainer_adam1.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "test_pred_adam1.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26fc094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'corporation': {'B-label': 0.23762376237623764,\n",
      "                        'I-label': 0.2857142857142857,\n",
      "                        'entity': 0.18333333333333332},\n",
      "        'creative-work': {'B-label': 0.36756756756756753,\n",
      "                          'I-label': 0.31016042780748665,\n",
      "                          'entity': 0.2772277227722772},\n",
      "        'group': {'B-label': 0.20960698689956328,\n",
      "                  'I-label': 0.17073170731707318,\n",
      "                  'entity': 0.16260162601626016},\n",
      "        'location': {'B-label': 0.5490196078431373,\n",
      "                     'I-label': 0.4516129032258065,\n",
      "                     'entity': 0.4866920152091255},\n",
      "        'person': {'B-label': 0.6015267175572518,\n",
      "                   'I-label': 0.46694214876033063,\n",
      "                   'entity': 0.5744047619047619},\n",
      "        'product': {'B-label': 0.28402366863905326,\n",
      "                    'I-label': 0.14973262032085563,\n",
      "                    'entity': 0.12807881773399013}},\n",
      " 'macro_f1': 0.30205637949495806,\n",
      " 'micro_f1': 0.3836747543253704,\n",
      " 'precision': {'corporation': {'B-label': 0.34285714285714286,\n",
      "                               'I-label': 0.32558139534883723,\n",
      "                               'entity': 0.2037037037037037},\n",
      "               'creative-work': {'B-label': 0.7906976744186046,\n",
      "                                 'I-label': 0.5,\n",
      "                                 'entity': 0.4666666666666667},\n",
      "               'group': {'B-label': 0.375,\n",
      "                         'I-label': 0.27450980392156865,\n",
      "                         'entity': 0.24691358024691357},\n",
      "               'location': {'B-label': 0.6666666666666666,\n",
      "                            'I-label': 0.5833333333333334,\n",
      "                            'entity': 0.5663716814159292},\n",
      "               'person': {'B-label': 0.8716814159292036,\n",
      "                          'I-label': 0.7635135135135135,\n",
      "                          'entity': 0.7942386831275721},\n",
      "               'product': {'B-label': 0.5714285714285714,\n",
      "                           'I-label': 0.21212121212121213,\n",
      "                           'entity': 0.17105263157894737}},\n",
      " 'recall': {'corporation': {'B-label': 0.18181818181818182,\n",
      "                            'I-label': 0.2545454545454545,\n",
      "                            'entity': 0.16666666666666666},\n",
      "            'creative-work': {'B-label': 0.23943661971830985,\n",
      "                              'I-label': 0.2248062015503876,\n",
      "                              'entity': 0.19718309859154928},\n",
      "            'group': {'B-label': 0.14545454545454545,\n",
      "                      'I-label': 0.12389380530973451,\n",
      "                      'entity': 0.12121212121212122},\n",
      "            'location': {'B-label': 0.4666666666666667,\n",
      "                         'I-label': 0.3684210526315789,\n",
      "                         'entity': 0.4266666666666667},\n",
      "            'person': {'B-label': 0.4592074592074592,\n",
      "                       'I-label': 0.33630952380952384,\n",
      "                       'entity': 0.44988344988344986},\n",
      "            'product': {'B-label': 0.1889763779527559,\n",
      "                        'I-label': 0.11570247933884298,\n",
      "                        'entity': 0.10236220472440945}}}\n"
     ]
    }
   ],
   "source": [
    "macro_micro_adam1 = evaluate_token_classification(logits=test_pred_adam1.predictions, \\\n",
    "                                                  labels=test_pred_adam1.label_ids, \\\n",
    "                                                  entity_types=entity_types)\n",
    "pprint(macro_micro_adam1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b0e6e",
   "metadata": {},
   "source": [
    "We directly observe an improvement in precision and recall.\n",
    "\n",
    "We continue by manually specifying a larger batch size. While this will speed up computation, it may also harm generalisability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "470bb272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 09:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218350</td>\n",
       "      <td>0.412844</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.886379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195233</td>\n",
       "      <td>0.510588</td>\n",
       "      <td>0.259569</td>\n",
       "      <td>0.344171</td>\n",
       "      <td>0.903238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.187051</td>\n",
       "      <td>0.521891</td>\n",
       "      <td>0.356459</td>\n",
       "      <td>0.423596</td>\n",
       "      <td>0.912550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arsen/miniconda3/envs/text_min/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.22683948278427124,\n",
       " 'test_precision': 0.47680890538033394,\n",
       " 'test_recall': 0.2381835032437442,\n",
       " 'test_f1': 0.31767614338689737,\n",
       " 'test_accuracy': 0.9293467336683418,\n",
       " 'test_runtime': 136.6502,\n",
       " 'test_samples_per_second': 9.418,\n",
       " 'test_steps_per_second': 1.178}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "optimizer2 = AdamW(model.parameters(), lr=2e-5)\n",
    "args_adam2 = TrainingArguments(\n",
    "    \"bert-finetuned-adam2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    ")\n",
    "\n",
    "trainer_adam2 = Trainer(\n",
    "    model=model,\n",
    "    args=args_adam2,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer2, None)\n",
    ")\n",
    "\n",
    "trainer_adam2.train()\n",
    "test_pred_adam2 = trainer_adam2.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "test_pred_adam2.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ce4af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arsen/miniconda3/envs/text_min/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'corporation': {'B-label': 0.057971014492753624,\n",
      "                        'I-label': 0.03571428571428572,\n",
      "                        'entity': 0.028985507246376812},\n",
      "        'creative-work': {'B-label': 0.0,\n",
      "                          'I-label': 0.02564102564102564,\n",
      "                          'entity': 0.08284023668639053},\n",
      "        'group': {'B-label': 0.0790960451977401,\n",
      "                  'I-label': 0.046875,\n",
      "                  'entity': 0.07446808510638299},\n",
      "        'location': {'B-label': 0.5035971223021581,\n",
      "                     'I-label': 0.3602484472049689,\n",
      "                     'entity': 0.42657342657342656},\n",
      "        'person': {'B-label': 0.4680851063829786,\n",
      "                   'I-label': 0.30268199233716475,\n",
      "                   'entity': 0.5071633237822349},\n",
      "        'product': {'B-label': 0.0,\n",
      "                    'I-label': 0.029850746268656716,\n",
      "                    'entity': 0.038461538461538464}},\n",
      " 'macro_f1': 0.1930820196427251,\n",
      " 'micro_f1': 0.2895340930523538,\n",
      " 'precision': {'corporation': {'B-label': 0.6666666666666666,\n",
      "                               'I-label': 1.0,\n",
      "                               'entity': 0.3333333333333333},\n",
      "               'creative-work': {'B-label': 0.0,\n",
      "                                 'I-label': 0.07407407407407407,\n",
      "                                 'entity': 0.25925925925925924},\n",
      "               'group': {'B-label': 0.5833333333333334,\n",
      "                         'I-label': 0.2,\n",
      "                         'entity': 0.30434782608695654},\n",
      "               'location': {'B-label': 0.546875,\n",
      "                            'I-label': 0.4393939393939394,\n",
      "                            'entity': 0.4485294117647059},\n",
      "               'person': {'B-label': 0.7857142857142857,\n",
      "                          'I-label': 0.42473118279569894,\n",
      "                          'entity': 0.6579925650557621},\n",
      "               'product': {'B-label': 0.0,\n",
      "                           'I-label': 0.0375,\n",
      "                           'entity': 0.04938271604938271}},\n",
      " 'recall': {'corporation': {'B-label': 0.030303030303030304,\n",
      "                            'I-label': 0.01818181818181818,\n",
      "                            'entity': 0.015151515151515152},\n",
      "            'creative-work': {'B-label': 0.0,\n",
      "                              'I-label': 0.015503875968992248,\n",
      "                              'entity': 0.04929577464788732},\n",
      "            'group': {'B-label': 0.04242424242424243,\n",
      "                      'I-label': 0.02654867256637168,\n",
      "                      'entity': 0.04242424242424243},\n",
      "            'location': {'B-label': 0.4666666666666667,\n",
      "                         'I-label': 0.30526315789473685,\n",
      "                         'entity': 0.4066666666666667},\n",
      "            'person': {'B-label': 0.3333333333333333,\n",
      "                       'I-label': 0.23511904761904762,\n",
      "                       'entity': 0.4125874125874126},\n",
      "            'product': {'B-label': 0.0,\n",
      "                        'I-label': 0.024793388429752067,\n",
      "                        'entity': 0.031496062992125984}}}\n"
     ]
    }
   ],
   "source": [
    "macro_micro_adam2 = evaluate_token_classification(logits=test_pred_adam2.predictions, \\\n",
    "                                                  labels=test_pred_adam2.label_ids, \\\n",
    "                                                  entity_types=entity_types)\n",
    "pprint(macro_micro_adam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c528b",
   "metadata": {},
   "source": [
    "Indeed, the model took less time to run 3 epochs, however suffered in performance. Particularly recall and thus F1 were harmed.\n",
    "\n",
    "Next, we increase the learning rate. This may also have a positive impact on computation time, while also acting as regularisation. Perhaps, the impact will not be as dire as with batch computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d51bd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1275' max='1275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1275/1275 14:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173663</td>\n",
       "      <td>0.555992</td>\n",
       "      <td>0.338517</td>\n",
       "      <td>0.420818</td>\n",
       "      <td>0.913192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.182855</td>\n",
       "      <td>0.624088</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.494220</td>\n",
       "      <td>0.919026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.223293</td>\n",
       "      <td>0.596748</td>\n",
       "      <td>0.438995</td>\n",
       "      <td>0.505858</td>\n",
       "      <td>0.921327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.2652566432952881,\n",
       " 'test_precision': 0.5217391304347826,\n",
       " 'test_recall': 0.3002780352177943,\n",
       " 'test_f1': 0.38117647058823534,\n",
       " 'test_accuracy': 0.932889447236181,\n",
       " 'test_runtime': 88.0235,\n",
       " 'test_samples_per_second': 14.621,\n",
       " 'test_steps_per_second': 1.829}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "optimizer3 = AdamW(model.parameters(), lr=9e-5)\n",
    "args_adam3 = TrainingArguments(\n",
    "    \"bert-finetuned-adam3\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer_adam3 = Trainer(\n",
    "    model=model,\n",
    "    args=args_adam3,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer3, None)\n",
    ")\n",
    "\n",
    "trainer_adam3.train()\n",
    "test_pred_adam3 = trainer_adam3.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "test_pred_adam3.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "775dc5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'corporation': {'B-label': 0.15789473684210525,\n",
      "                        'I-label': 0.1836734693877551,\n",
      "                        'entity': 0.109375},\n",
      "        'creative-work': {'B-label': 0.40625,\n",
      "                          'I-label': 0.3191489361702128,\n",
      "                          'entity': 0.3121951219512195},\n",
      "        'group': {'B-label': 0.2882096069868996,\n",
      "                  'I-label': 0.20118343195266272,\n",
      "                  'entity': 0.2390438247011952},\n",
      "        'location': {'B-label': 0.5783132530120482,\n",
      "                     'I-label': 0.4383561643835617,\n",
      "                     'entity': 0.5},\n",
      "        'person': {'B-label': 0.5924812030075188,\n",
      "                   'I-label': 0.4512195121951219,\n",
      "                   'entity': 0.5364431486880467},\n",
      "        'product': {'B-label': 0.15789473684210523,\n",
      "                    'I-label': 0.10126582278481013,\n",
      "                    'entity': 0.08045977011494251}},\n",
      " 'macro_f1': 0.296252810909234,\n",
      " 'micro_f1': 0.37659424465666363,\n",
      " 'precision': {'corporation': {'B-label': 0.1875,\n",
      "                               'I-label': 0.20930232558139536,\n",
      "                               'entity': 0.11290322580645161},\n",
      "               'creative-work': {'B-label': 0.78,\n",
      "                                 'I-label': 0.5084745762711864,\n",
      "                                 'entity': 0.5079365079365079},\n",
      "               'group': {'B-label': 0.515625,\n",
      "                         'I-label': 0.30357142857142855,\n",
      "                         'entity': 0.3488372093023256},\n",
      "               'location': {'B-label': 0.7272727272727273,\n",
      "                            'I-label': 0.6274509803921569,\n",
      "                            'entity': 0.6037735849056604},\n",
      "               'person': {'B-label': 0.8347457627118644,\n",
      "                          'I-label': 0.7115384615384616,\n",
      "                          'entity': 0.7159533073929961},\n",
      "               'product': {'B-label': 0.48,\n",
      "                           'I-label': 0.21621621621621623,\n",
      "                           'entity': 0.14893617021276595}},\n",
      " 'recall': {'corporation': {'B-label': 0.13636363636363635,\n",
      "                            'I-label': 0.16363636363636364,\n",
      "                            'entity': 0.10606060606060606},\n",
      "            'creative-work': {'B-label': 0.2746478873239437,\n",
      "                              'I-label': 0.23255813953488372,\n",
      "                              'entity': 0.22535211267605634},\n",
      "            'group': {'B-label': 0.2,\n",
      "                      'I-label': 0.1504424778761062,\n",
      "                      'entity': 0.18181818181818182},\n",
      "            'location': {'B-label': 0.48,\n",
      "                         'I-label': 0.3368421052631579,\n",
      "                         'entity': 0.4266666666666667},\n",
      "            'person': {'B-label': 0.4592074592074592,\n",
      "                       'I-label': 0.33035714285714285,\n",
      "                       'entity': 0.4289044289044289},\n",
      "            'product': {'B-label': 0.09448818897637795,\n",
      "                        'I-label': 0.06611570247933884,\n",
      "                        'entity': 0.05511811023622047}}}\n"
     ]
    }
   ],
   "source": [
    "macro_micro_adam3 = evaluate_token_classification(logits=test_pred_adam3.predictions, \\\n",
    "                                                  labels=test_pred_adam3.label_ids, \\\n",
    "                                                  entity_types=entity_types)\n",
    "pprint(macro_micro_adam3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc5c77",
   "metadata": {},
   "source": [
    "The model actually has better precision than the baseline. However, recall still underperforms, leading to an overall lower F1. It appears that the true positives of the test data do not resemble the training data too well, making the recall worse.\n",
    "We will try the same learning rate, including the increased batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae9b2a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 09:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.204903</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.902810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.177497</td>\n",
       "      <td>0.562278</td>\n",
       "      <td>0.377990</td>\n",
       "      <td>0.452074</td>\n",
       "      <td>0.915547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173086</td>\n",
       "      <td>0.524725</td>\n",
       "      <td>0.456938</td>\n",
       "      <td>0.488491</td>\n",
       "      <td>0.922451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arsen/miniconda3/envs/text_min/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.22278931736946106,\n",
       " 'test_precision': 0.45241379310344826,\n",
       " 'test_recall': 0.303985171455051,\n",
       " 'test_f1': 0.3636363636363637,\n",
       " 'test_accuracy': 0.9338442211055277,\n",
       " 'test_runtime': 140.9482,\n",
       " 'test_samples_per_second': 9.131,\n",
       " 'test_steps_per_second': 1.142}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "optimizer4 = AdamW(model.parameters(), lr=9e-5)\n",
    "args_adam4 = TrainingArguments(\n",
    "    \"bert-finetuned-adam4\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    ")\n",
    "\n",
    "trainer_adam4 = Trainer(\n",
    "    model=model,\n",
    "    args=args_adam4,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer4, None)\n",
    ")\n",
    "\n",
    "trainer_adam4.train()\n",
    "test_pred_adam4 = trainer_adam4.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "test_pred_adam4.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71a7fb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'corporation': {'B-label': 0.2580645161290323,\n",
      "                        'I-label': 0.1769911504424779,\n",
      "                        'entity': 0.1456953642384106},\n",
      "        'creative-work': {'B-label': 0.36180904522613067,\n",
      "                          'I-label': 0.25555555555555554,\n",
      "                          'entity': 0.22535211267605632},\n",
      "        'group': {'B-label': 0.29310344827586204,\n",
      "                  'I-label': 0.1807909604519774,\n",
      "                  'entity': 0.19696969696969696},\n",
      "        'location': {'B-label': 0.6039215686274509,\n",
      "                     'I-label': 0.43137254901960786,\n",
      "                     'entity': 0.5037593984962406},\n",
      "        'person': {'B-label': 0.5961820851688693,\n",
      "                   'I-label': 0.46613545816733065,\n",
      "                   'entity': 0.5436337625178828},\n",
      "        'product': {'B-label': 0.32183908045977005,\n",
      "                    'I-label': 0.11640211640211641,\n",
      "                    'entity': 0.0947867298578199}},\n",
      " 'macro_f1': 0.2850328441260179,\n",
      " 'micro_f1': 0.3660209477537405,\n",
      " 'precision': {'corporation': {'B-label': 0.27586206896551724,\n",
      "                               'I-label': 0.1724137931034483,\n",
      "                               'entity': 0.12941176470588237},\n",
      "               'creative-work': {'B-label': 0.631578947368421,\n",
      "                                 'I-label': 0.45098039215686275,\n",
      "                                 'entity': 0.3380281690140845},\n",
      "               'group': {'B-label': 0.5074626865671642,\n",
      "                         'I-label': 0.25,\n",
      "                         'entity': 0.26262626262626265},\n",
      "               'location': {'B-label': 0.7333333333333333,\n",
      "                            'I-label': 0.5689655172413793,\n",
      "                            'entity': 0.5775862068965517},\n",
      "               'person': {'B-label': 0.8055555555555556,\n",
      "                          'I-label': 0.7048192771084337,\n",
      "                          'entity': 0.7037037037037037},\n",
      "               'product': {'B-label': 0.5957446808510638,\n",
      "                           'I-label': 0.16176470588235295,\n",
      "                           'entity': 0.11904761904761904}},\n",
      " 'recall': {'corporation': {'B-label': 0.24242424242424243,\n",
      "                            'I-label': 0.18181818181818182,\n",
      "                            'entity': 0.16666666666666666},\n",
      "            'creative-work': {'B-label': 0.2535211267605634,\n",
      "                              'I-label': 0.17829457364341086,\n",
      "                              'entity': 0.16901408450704225},\n",
      "            'group': {'B-label': 0.20606060606060606,\n",
      "                      'I-label': 0.1415929203539823,\n",
      "                      'entity': 0.15757575757575756},\n",
      "            'location': {'B-label': 0.5133333333333333,\n",
      "                         'I-label': 0.3473684210526316,\n",
      "                         'entity': 0.44666666666666666},\n",
      "            'person': {'B-label': 0.4731934731934732,\n",
      "                       'I-label': 0.3482142857142857,\n",
      "                       'entity': 0.4428904428904429},\n",
      "            'product': {'B-label': 0.2204724409448819,\n",
      "                        'I-label': 0.09090909090909091,\n",
      "                        'entity': 0.07874015748031496}}}\n"
     ]
    }
   ],
   "source": [
    "macro_micro_adam4 = evaluate_token_classification(logits=test_pred_adam4.predictions, \\\n",
    "                                                  labels=test_pred_adam4.label_ids, \\\n",
    "                                                  entity_types=entity_types)\n",
    "pprint(macro_micro_adam4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276cdc4",
   "metadata": {},
   "source": [
    "As it has proven challenging to improve the model, we will also try a lower learning rate than at baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f36d4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1275' max='1275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1275/1275 14:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184176</td>\n",
       "      <td>0.580097</td>\n",
       "      <td>0.285885</td>\n",
       "      <td>0.383013</td>\n",
       "      <td>0.908108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.178682</td>\n",
       "      <td>0.597070</td>\n",
       "      <td>0.389952</td>\n",
       "      <td>0.471780</td>\n",
       "      <td>0.918598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.166287</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.440191</td>\n",
       "      <td>0.497297</td>\n",
       "      <td>0.920792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arsen/miniconda3/envs/text_min/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.2242525964975357,\n",
       " 'test_precision': 0.4936102236421725,\n",
       " 'test_recall': 0.28637627432808155,\n",
       " 'test_f1': 0.3624633431085044,\n",
       " 'test_accuracy': 0.9333668341708543,\n",
       " 'test_runtime': 89.0857,\n",
       " 'test_samples_per_second': 14.447,\n",
       " 'test_steps_per_second': 1.807}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "optimizer5 = AdamW(model.parameters(), lr=1e-5)\n",
    "args_adam5 = TrainingArguments(\n",
    "    \"bert-finetuned-adam5\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer_adam5 = Trainer(\n",
    "    model=model,\n",
    "    args=args_adam5,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer5, None)\n",
    ")\n",
    "\n",
    "trainer_adam5.train()\n",
    "test_pred_adam5 = trainer_adam5.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "test_pred_adam5.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31a40391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'corporation': {'B-label': 0.2456140350877193,\n",
      "                        'I-label': 0.2772277227722772,\n",
      "                        'entity': 0.17777777777777776},\n",
      "        'creative-work': {'B-label': 0.11688311688311688,\n",
      "                          'I-label': 0.14893617021276598,\n",
      "                          'entity': 0.17910447761194032},\n",
      "        'group': {'B-label': 0.16494845360824745,\n",
      "                  'I-label': 0.0979020979020979,\n",
      "                  'entity': 0.13333333333333333},\n",
      "        'location': {'B-label': 0.5681818181818182,\n",
      "                     'I-label': 0.4645161290322581,\n",
      "                     'entity': 0.48905109489051096},\n",
      "        'person': {'B-label': 0.5885978428351311,\n",
      "                   'I-label': 0.4630738522954092,\n",
      "                   'entity': 0.5568513119533529},\n",
      "        'product': {'B-label': 0.11267605633802817,\n",
      "                    'I-label': 0.06382978723404256,\n",
      "                    'entity': 0.07035175879396985}},\n",
      " 'macro_f1': 0.26774495906014756,\n",
      " 'micro_f1': 0.3525002035056794,\n",
      " 'precision': {'corporation': {'B-label': 0.2916666666666667,\n",
      "                               'I-label': 0.30434782608695654,\n",
      "                               'entity': 0.17391304347826086},\n",
      "               'creative-work': {'B-label': 0.75,\n",
      "                                 'I-label': 0.23728813559322035,\n",
      "                                 'entity': 0.3050847457627119},\n",
      "               'group': {'B-label': 0.5517241379310345,\n",
      "                         'I-label': 0.23333333333333334,\n",
      "                         'entity': 0.3111111111111111},\n",
      "               'location': {'B-label': 0.6578947368421053,\n",
      "                            'I-label': 0.6,\n",
      "                            'entity': 0.5403225806451613},\n",
      "               'person': {'B-label': 0.8681818181818182,\n",
      "                          'I-label': 0.703030303030303,\n",
      "                          'entity': 0.7431906614785992},\n",
      "               'product': {'B-label': 0.5333333333333333,\n",
      "                           'I-label': 0.08955223880597014,\n",
      "                           'entity': 0.09722222222222222}},\n",
      " 'recall': {'corporation': {'B-label': 0.21212121212121213,\n",
      "                            'I-label': 0.2545454545454545,\n",
      "                            'entity': 0.18181818181818182},\n",
      "            'creative-work': {'B-label': 0.06338028169014084,\n",
      "                              'I-label': 0.10852713178294573,\n",
      "                              'entity': 0.1267605633802817},\n",
      "            'group': {'B-label': 0.09696969696969697,\n",
      "                      'I-label': 0.061946902654867256,\n",
      "                      'entity': 0.08484848484848485},\n",
      "            'location': {'B-label': 0.5,\n",
      "                         'I-label': 0.37894736842105264,\n",
      "                         'entity': 0.44666666666666666},\n",
      "            'person': {'B-label': 0.44522144522144524,\n",
      "                       'I-label': 0.34523809523809523,\n",
      "                       'entity': 0.44522144522144524},\n",
      "            'product': {'B-label': 0.06299212598425197,\n",
      "                        'I-label': 0.049586776859504134,\n",
      "                        'entity': 0.05511811023622047}}}\n"
     ]
    }
   ],
   "source": [
    "macro_micro_adam5 = evaluate_token_classification(logits=test_pred_adam5.predictions, \\\n",
    "                                                  labels=test_pred_adam5.label_ids, \\\n",
    "                                                  entity_types=entity_types)\n",
    "pprint(macro_micro_adam5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0f4a4",
   "metadata": {},
   "source": [
    "The decrease in model performance is negligible.\n",
    "Again, we also try to increase the batch size to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fdd7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 08:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220329</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>0.092975</td>\n",
       "      <td>0.889323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.211092</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>0.160287</td>\n",
       "      <td>0.241877</td>\n",
       "      <td>0.897458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arsen/miniconda3/envs/text_min/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/arsen/miniconda3/envs/text_min/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.230812668800354,\n",
       " 'test_precision': 0.46616541353383456,\n",
       " 'test_recall': 0.11492122335495829,\n",
       " 'test_f1': 0.18438661710037177,\n",
       " 'test_accuracy': 0.922713567839196,\n",
       " 'test_runtime': 140.0568,\n",
       " 'test_samples_per_second': 9.189,\n",
       " 'test_steps_per_second': 1.15}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "optimizer6 = AdamW(model.parameters(), lr=1e-5)\n",
    "args_adam6 = TrainingArguments(\n",
    "    \"bert-finetuned-adam6\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    ")\n",
    "\n",
    "trainer_adam6 = Trainer(\n",
    "    model=model,\n",
    "    args=args_adam6,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer6, None)\n",
    ")\n",
    "\n",
    "trainer_adam6.train()\n",
    "test_pred_adam6 = trainer_adam6.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "test_pred_adam6.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8ae195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'corporation': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "        'creative-work': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "        'group': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "        'location': {'B-label': 0.05063291139240507,\n",
      "                     'I-label': 0.06451612903225808,\n",
      "                     'entity': 0.22119815668202766},\n",
      "        'person': {'B-label': 0.2589641434262948,\n",
      "                   'I-label': 0.21399176954732513,\n",
      "                   'entity': 0.3333333333333333},\n",
      "        'product': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0}},\n",
      " 'macro_f1': 0.09242191500256015,\n",
      " 'micro_f1': 0.16328055931631524,\n",
      " 'precision': {'corporation': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "               'creative-work': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "               'group': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "               'location': {'B-label': 0.5,\n",
      "                            'I-label': 0.08333333333333333,\n",
      "                            'entity': 0.3582089552238806},\n",
      "               'person': {'B-label': 0.8904109589041096,\n",
      "                          'I-label': 0.3466666666666667,\n",
      "                          'entity': 0.5847953216374269},\n",
      "               'product': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0}},\n",
      " 'recall': {'corporation': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "            'creative-work': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "            'group': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0},\n",
      "            'location': {'B-label': 0.02666666666666667,\n",
      "                         'I-label': 0.05263157894736842,\n",
      "                         'entity': 0.16},\n",
      "            'person': {'B-label': 0.15151515151515152,\n",
      "                       'I-label': 0.15476190476190477,\n",
      "                       'entity': 0.2331002331002331},\n",
      "            'product': {'B-label': 0.0, 'I-label': 0.0, 'entity': 0.0}}}\n"
     ]
    }
   ],
   "source": [
    "macro_micro_adam6 = evaluate_token_classification(logits=test_pred_adam6.predictions, \\\n",
    "                                                  labels=test_pred_adam6.label_ids, \\\n",
    "                                                  entity_types=entity_types)\n",
    "pprint(macro_micro_adam6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfba00",
   "metadata": {},
   "source": [
    "This model struggles a lot to correctly identify true positives from false negatives, leading to a terrible Recall and F1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_min",
   "language": "python",
   "name": "text_min"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
